{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box, Dict, Tuple, MultiBinary, MultiDiscrete \n",
    "\n",
    "from stable_baselines3.ppo.ppo import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "\n",
    "from mtj_config_RL import mtj_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build MTJ Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTJ_Env(Env):\n",
    "  def __init__(self):\n",
    "    # alpha_vals   = [0.01, 0.03, 0.05, 0.07, 0.1]            # damping constant\n",
    "    # Ki_vals      = [0.2e-3, 0.4e-3, 0.6e-3, 0.8e-3, 1e-3]   # anistrophy energy    \n",
    "    # Ms_vals      = [0.3e6, 0.7e6, 1.2e6, 1.6e6, 2e6]        # saturation magnetization\n",
    "    # Rp_vals      = [500, 1000, 5000, 25000, 50000]          # parallel resistance\n",
    "    # TMR_vals     = [0.3, 0.5, 2, 4, 6]                      # tunneling magnetoresistance ratio\n",
    "    # eta_vals     = [0.1, 0.2, 0.4, 0.6, 0.8]                # spin hall angle\n",
    "    # J_she_vals   = [0.01e12, 0.1e12, 0.25e12, 0.5e12, 1e12] # current density\n",
    "    # d_vals       = [50]                                     # free layer diameter\n",
    "    # tf_vals      = [1.1]                                    # free layer thickness\n",
    "\n",
    "    # Initial parameter values\n",
    "    self.alpha = 0.05\n",
    "    self.Ki = 0.6e-3\n",
    "    self.Ms = 1.2e6\n",
    "    self.Rp = 5000\n",
    "    self.TMR = 2\n",
    "    self.eta = 0.4\n",
    "    self.J_she = 0.25e12\n",
    "    self.d = 50\n",
    "    self.tf = 1.1\n",
    "\n",
    "    # Parameter ranges\n",
    "    self.alpha_range = [0.01, 0.1]\n",
    "    self.Ki_range = [0.2e-3, 1e-3]\n",
    "    self.Ms_range = [0.3e6, 2e6]\n",
    "    self.Rp_range = [500, 50000]\n",
    "    self.TMR_range = [0.3, 6]\n",
    "    self.eta_range = [0.1, 0.8]\n",
    "    self.J_she_range = [0.01e12, 1e12]\n",
    "\n",
    "    # Parameter step sizes\n",
    "    self.alpha_step = 0.01\n",
    "    self.Ki_step = 0.1e-3\n",
    "    self.Ms_step = 0.1e6\n",
    "    self.Rp_step = 500\n",
    "    self.TMR_step = 0.5\n",
    "    self.eta_step = 0.4\n",
    "    self.J_she_step = 0.25e12\n",
    "\n",
    "    # Actions: increase/decrease the 7 parameters\n",
    "    self.action_space = Discrete(14)\n",
    "    \n",
    "    # Observation array\n",
    "    self.observation_space = Box(low=np.array([0]), high=np.array([100]))\n",
    "    \n",
    "    # Set initial state\n",
    "    # chi2, bitstream, energy_avg, countData, bitData, magTheta, magPhi = mtj_run(self.alpha, self.Ki, self.Ms, self.Rp, self.TMR, self.d, self.tf, self.eta, self.J_she, run=0)\n",
    "    # self.state = np.mean(energy_avg)\n",
    "    self.state = np.array([2.98751468577824e-13], dtype=float)\n",
    "    self.prev_state = self.state\n",
    "    \n",
    "    # Set episode length\n",
    "    self.episode_length = 60\n",
    "  \n",
    "\n",
    "  def apply_action(self, action):\n",
    "    if action == 0:\n",
    "      temp = self.alpha + self.alpha_step\n",
    "      self.alpha = min(temp, self.alpha_range[1])\n",
    "    elif action == 1:\n",
    "      temp = self.alpha - self.alpha_step\n",
    "      self.alpha = max(temp, self.alpha_range[0])\n",
    "\n",
    "    elif action == 2:\n",
    "      temp = self.Ki + self.Ki_step\n",
    "      self.Ki = min(temp, self.Ki_range[1])\n",
    "    elif action == 3:\n",
    "      temp = self.Ki - self.Ki_step\n",
    "      self.Ki = max(temp, self.Ki_range[0])\n",
    "    \n",
    "    elif action == 4:\n",
    "      temp = self.Ms + self.Ms_step\n",
    "      self.Ms = min(temp, self.Ms_range[1])\n",
    "    elif action == 5:\n",
    "      temp = self.Ms - self.Ms_step\n",
    "      self.Ms = max(temp, self.Ms_range[0])\n",
    "    \n",
    "    elif action == 6:\n",
    "      temp = self.Rp + self.Rp_step\n",
    "      self.Rp = min(temp, self.Rp_range[1])\n",
    "    elif action == 7:\n",
    "      temp = self.Rp - self.Rp_step\n",
    "      self.Rp = max(temp, self.Rp_range[0])\n",
    "    \n",
    "    elif action == 8:\n",
    "      temp = self.TMR + self.TMR_step\n",
    "      self.TMR = min(temp, self.TMR_range[1])\n",
    "    elif action == 9:\n",
    "      temp = self.TMR - self.TMR_step\n",
    "      self.TMR = max(temp, self.TMR_range[0])\n",
    "    \n",
    "    elif action == 10:\n",
    "      temp = self.eta + self.eta_step\n",
    "      self.eta = min(temp, self.eta_range[1])\n",
    "    elif action == 11:\n",
    "      temp = self.eta - self.eta_step\n",
    "      self.eta = max(temp, self.eta_range[0])\n",
    "    \n",
    "    elif action == 12:\n",
    "      temp = self.J_she + self.J_she_step\n",
    "      self.J_she = min(temp, self.J_she_range[1])\n",
    "    elif action == 13:\n",
    "      temp = self.J_she - self.J_she_step\n",
    "      self.J_she = max(temp, self.J_she_range[0])\n",
    "\n",
    "\n",
    "  def reward_function(self):\n",
    "    if self.state < self.prev_state:\n",
    "      reward = 1 \n",
    "    else: \n",
    "      reward = -1 \n",
    "    return reward\n",
    "\n",
    "\n",
    "  def step(self, action):\n",
    "    # Apply action\n",
    "    self.apply_action(action)\n",
    "    \n",
    "    # Sample new configuration\n",
    "    chi2, bitstream, energy_avg, countData, bitData, magTheta, magPhi = mtj_run(self.alpha, self.Ki, self.Ms, self.Rp, self.TMR, self.d, self.tf, self.eta, self.J_she, run=0)\n",
    "    self.prev_state = self.state\n",
    "    self.state = np.array([np.mean(energy_avg)], dtype=float)\n",
    "    \n",
    "    # Reduce episode length\n",
    "    self.episode_length -= 1 \n",
    "    \n",
    "    # Calculate reward\n",
    "    reward = self.reward_function()\n",
    "    \n",
    "    # Check if episode is done\n",
    "    if self.episode_length <= 0: \n",
    "      done = True\n",
    "    else:\n",
    "      done = False\n",
    "    \n",
    "    # Set placeholder for info\n",
    "    self.info = {'alpha': self.alpha,\n",
    "                'Ki'    : self.Ki,\n",
    "                'Ms'    : self.Ms,\n",
    "                'Rp'    : self.Rp,\n",
    "                'TMR'   : self.TMR,\n",
    "                'eta'   : self.eta,\n",
    "                'J_she' : self.J_she}\n",
    "    \n",
    "    # Return step information\n",
    "    return self.state, reward, done, self.info\n",
    "\n",
    "\n",
    "  def render(self):\n",
    "    # Implement viz\n",
    "    pass\n",
    "  \n",
    "\n",
    "  def reset(self):\n",
    "    # Initial parameter values\n",
    "    self.alpha = 0.05\n",
    "    self.Ki = 0.6e-3\n",
    "    self.Ms = 1.2e6\n",
    "    self.Rp = 5000\n",
    "    self.TMR = 2\n",
    "    self.eta = 0.4\n",
    "    self.J_she = 0.25e12\n",
    "    self.d = 50\n",
    "    self.tf = 1.1\n",
    "    \n",
    "    # chi2, bitstream, energy_avg, countData, bitData, magTheta, magPhi = mtj_run(self.alpha, self.Ki, self.Ms, self.Rp, self.TMR, self.d, self.tf, self.eta, self.J_she, run=0)\n",
    "    # self.state = np.mean(energy_avg)\n",
    "    self.state = np.array([2.98751468577824e-13], dtype=float)\n",
    "    self.prev_state = self.state\n",
    "    self.episode_length = 60\n",
    "    \n",
    "    return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kpatel68/COINFLIPS/MTJ_env/lib/python3.8/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "env = MTJ_Env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:4\n",
      "Episode:2 Score:2\n",
      "Episode:3 Score:10\n",
      "Episode:4 Score:-4\n",
      "Episode:5 Score:0\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "  state = env.reset()\n",
    "  done = False\n",
    "  score = 0 \n",
    "  \n",
    "  while not done:\n",
    "    # env.render()\n",
    "    action = env.action_space.sample()\n",
    "    n_state, reward, done, info = env.step(action)\n",
    "    score += reward\n",
    "\n",
    "  print('Episode:{} Score:{}'.format(episode, score))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to Training/Logs/PPO_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kpatel68/COINFLIPS/MTJ_env/lib/python3.8/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 60       |\n",
      "|    ep_rew_mean     | 0.176    |\n",
      "| time/              |          |\n",
      "|    fps             | 0        |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6625     |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7ff1c283f850>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_path = os.path.join('Training', 'Logs')\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_path)\n",
    "model.learn(total_timesteps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join('Training', 'Saved_Models', 'mtj_model_PPO')\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4, 3.555277766926236)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kpatel68/COINFLIPS/MTJ_env/lib/python3.8/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-2\n",
      "Best Config:  {'alpha': 0.03, 'Ki': 0.0006, 'Ms': 900000.0, 'Rp': 4000, 'TMR': 1.3, 'eta': 0.5, 'J_she': 260000000000.0}\n",
      "Episode:2 Score:2\n",
      "Best Config:  {'alpha': 0.04, 'Ki': 0.0004999999999999999, 'Ms': 1400000.0, 'Rp': 5500, 'TMR': 2.5, 'eta': 0.5, 'J_she': 10000000000.0}\n",
      "Episode:3 Score:0\n",
      "Best Config:  {'alpha': 0.05, 'Ki': 0.0003999999999999999, 'Ms': 1300000.0, 'Rp': 5000, 'TMR': 3.5, 'eta': 0.8, 'J_she': 500000000000.0}\n",
      "Episode:4 Score:6\n",
      "Best Config:  {'alpha': 0.04, 'Ki': 0.0002999999999999999, 'Ms': 1300000.0, 'Rp': 4500, 'TMR': 1.5, 'eta': 0.4, 'J_she': 250000000000.0}\n",
      "Episode:5 Score:4\n",
      "Best Config:  {'alpha': 0.07, 'Ki': 0.0009000000000000001, 'Ms': 1200000.0, 'Rp': 4500, 'TMR': 3.0, 'eta': 0.8, 'J_she': 10000000000.0}\n",
      "Episode:6 Score:8\n",
      "Best Config:  {'alpha': 0.03, 'Ki': 0.0007, 'Ms': 1100000.0, 'Rp': 6500, 'TMR': 2.5, 'eta': 0.1, 'J_she': 10000000000.0}\n",
      "Episode:7 Score:2\n",
      "Best Config:  {'alpha': 0.060000000000000005, 'Ki': 0.0004999999999999999, 'Ms': 1300000.0, 'Rp': 5500, 'TMR': 1.5, 'eta': 0.4, 'J_she': 250000000000.0}\n",
      "Episode:8 Score:-8\n",
      "Best Config:  {'alpha': 0.04, 'Ki': 0.0007, 'Ms': 1100000.0, 'Rp': 4000, 'TMR': 4.5, 'eta': 0.8, 'J_she': 260000000000.0}\n",
      "Episode:9 Score:-6\n",
      "Best Config:  {'alpha': 0.05, 'Ki': 0.001, 'Ms': 1000000.0, 'Rp': 3500, 'TMR': 1.0, 'eta': 0.8, 'J_she': 510000000000.0}\n",
      "Episode:10 Score:2\n",
      "Best Config:  {'alpha': 0.04, 'Ki': 0.0006, 'Ms': 1200000.0, 'Rp': 5500, 'TMR': 1.0, 'eta': 0.8, 'J_she': 10000000000.0}\n"
     ]
    }
   ],
   "source": [
    "# del model\n",
    "model = PPO.load(\"PPO\")\n",
    "env = MTJ_Env()\n",
    "\n",
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "  state = env.reset()\n",
    "  done = False\n",
    "  score = 0\n",
    "  infos = []\n",
    "  energies = []\n",
    "  \n",
    "  while not done:\n",
    "    # env.render()\n",
    "    action, _states = model.predict(state)\n",
    "    n_state, reward, done, info = env.step(action)\n",
    "    score += reward\n",
    "\n",
    "    infos.append(info)\n",
    "    energies.append(env.state[0])\n",
    "\n",
    "  best_config = infos[np.argmin(energies)]\n",
    "  print('Episode:{} Score:{}'.format(episode, score))\n",
    "  print('Best Config: ', best_config)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MTJ_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
